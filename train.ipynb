{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4b66e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from transformers import BertModel, BertTokenizer, BertConfig, get_cosine_schedule_with_warmup\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
    "from torch.optim import AdamW\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "BERT_PATH = \"bert_model/\"    # 该文件夹下存放三个文件（'vocab.txt', 'pytorch_model.bin', 'config.json'）\n",
    "# DATA_PATH = \"data/tags_data.txt\"\n",
    "DATA_PATH = \"data/test_data.txt\"\n",
    "MAX_LEN = 64\n",
    "BATCH_SIZE = 4\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33e36253",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bert_Model(nn.Module):\n",
    "    def __init__(self, bert_path, classes=2):\n",
    "        super(Bert_Model, self).__init__()\n",
    "        self.config = BertConfig.from_pretrained(bert_path)  # 导入模型超参数\n",
    "        self.bert = BertModel.from_pretrained(bert_path)     # 加载预训练模型权重\n",
    "        self.fc = nn.Linear(self.config.hidden_size, classes)  # 直接分类\n",
    "        \n",
    "        \n",
    "    def forward(self, input_ids, attention_mask=None, token_type_ids=None):\n",
    "        outputs = self.bert(input_ids, attention_mask, token_type_ids)\n",
    "        out_pool = outputs[1]   # 池化\n",
    "        logit = self.fc(out_pool) # 线性模型二分类\n",
    "        return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35e41f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(BERT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe7dc1c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "746it [00:01, 644.27it/s]\n"
     ]
    }
   ],
   "source": [
    "def load_data():\n",
    "    input_ids, input_masks, input_types, tag_labels = [], [], [], []\n",
    "\n",
    "    with open(DATA_PATH, encoding=\"utf-8\") as f:\n",
    "        for line in tqdm(f):\n",
    "            tags, labels = line.strip().split(\"\\t\")\n",
    "            encode_dict = tokenizer.encode_plus(text=tags, max_length=MAX_LEN,\n",
    "                                                    padding=\"max_length\", truncation=True)\n",
    "\n",
    "            input_ids.append(encode_dict[\"input_ids\"])\n",
    "            input_types.append(encode_dict[\"token_type_ids\"])\n",
    "            input_masks.append(encode_dict[\"attention_mask\"])\n",
    "            tag_labels.append(int(labels))\n",
    "\n",
    "    all_data = (input_ids, input_masks, input_types, tag_labels)\n",
    "    unit = len(tag_labels) // 10\n",
    "    train_data = list(map(lambda x: x[:unit*8], all_data))\n",
    "    valid_data = list(map(lambda x: x[unit*8:unit*9], all_data))\n",
    "    test_data = list(map(lambda x: x[unit*9:], all_data))\n",
    "\n",
    "    return train_data, valid_data, test_data\n",
    "train_data, valid_data, test_data = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20512bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(*tuple(map(torch.LongTensor, train_data)))\n",
    "train_sampler = RandomSampler(train_dataset)  \n",
    "train_loader = DataLoader(train_dataset, sampler=train_sampler, batch_size=BATCH_SIZE)\n",
    "\n",
    "valid_dataset = TensorDataset(*tuple(map(torch.LongTensor, valid_data)))\n",
    "valid_sampler = RandomSampler(valid_dataset)  \n",
    "valid_loader = DataLoader(valid_dataset, sampler=valid_sampler, batch_size=BATCH_SIZE)\n",
    "\n",
    "test_dataset = TensorDataset(*tuple(map(torch.LongTensor, test_data)))\n",
    "test_sampler = RandomSampler(test_dataset)  \n",
    "test_loader = DataLoader(test_dataset, sampler=test_sampler, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "437a681e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert_model/ were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 102269186, Trainable parameters: 102269186\n"
     ]
    }
   ],
   "source": [
    "def get_parameter_number(model):\n",
    "    #  打印模型参数量\n",
    "    total_num = sum(p.numel() for p in model.parameters())\n",
    "    trainable_num = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return 'Total parameters: {}, Trainable parameters: {}'.format(total_num, trainable_num)\n",
    "\n",
    "# DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "DEVICE = torch.device(\"cpu\")\n",
    "model = Bert_Model(BERT_PATH).to(DEVICE)\n",
    "print(get_parameter_number(model))\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=1e-4) #AdamW优化器\n",
    "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=len(train_loader),\n",
    "                                            num_training_steps=EPOCHS*len(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d98dfcb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 评估模型性能，在验证集上\n",
    "def evaluate(model, data_loader, device):\n",
    "    model.eval()\n",
    "    val_true, val_pred = [], []\n",
    "    with torch.no_grad():\n",
    "        for idx, (ids, att, tpe, y) in (enumerate(data_loader)):\n",
    "            y_pred = model(ids.to(device), att.to(device), tpe.to(device))\n",
    "            y_pred = torch.argmax(y_pred, dim=1).detach().cpu().numpy().tolist()\n",
    "            val_pred.extend(y_pred)\n",
    "            val_true.extend(y.squeeze().cpu().numpy().tolist())\n",
    "    \n",
    "    return accuracy_score(val_true, val_pred)  #返回accuracy\n",
    "\n",
    "\n",
    "# 测试集没有标签，需要预测提交\n",
    "def predict(model, data_loader, device):\n",
    "    model.eval()\n",
    "    val_true, val_pred = [], []\n",
    "    with torch.no_grad():\n",
    "        for idx, (ids, att, tpe, y) in tqdm(enumerate(data_loader)):\n",
    "            y_pred = model(ids.to(device), att.to(device), tpe.to(device))\n",
    "            y_pred = torch.argmax(y_pred, dim=1).detach().cpu().numpy().tolist()\n",
    "            val_pred.extend(y_pred)\n",
    "            val_true.extend(y.squeeze().cpu().numpy().tolist())\n",
    "\n",
    "    print(\"\\n Test Accuracy = {} \\n\".format(accuracy_score(val_true, val_pred)))\n",
    "    print(classification_report(val_true, val_pred, digits=4))\n",
    "\n",
    "\n",
    "def train_and_eval(model, train_loader, valid_loader, \n",
    "                   optimizer, scheduler, device, epoch):\n",
    "    best_acc = 0.0\n",
    "    patience = 0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    for i in range(epoch):\n",
    "        \"\"\"训练模型\"\"\"\n",
    "        start = time.time()\n",
    "        model.train()\n",
    "        print(\"***** Running training epoch {} *****\".format(i+1))\n",
    "        train_loss_sum = 0.0\n",
    "        for idx, (ids, att, tpe, y) in enumerate(train_loader):\n",
    "            ids, att, tpe, y = ids.to(device), att.to(device), tpe.to(device), y.to(device)  \n",
    "            y_pred = model(ids, att, tpe)\n",
    "            loss = criterion(y_pred, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()   # 学习率变化\n",
    "            \n",
    "            train_loss_sum += loss.item()\n",
    "            if (idx + 1) % (len(train_loader)//5) == 0:    # 只打印五次结果\n",
    "                print(\"Epoch {:04d} | Step {:04d}/{:04d} | Loss {:.4f} | Time {:.4f}\".format(\n",
    "                          i+1, idx+1, len(train_loader), train_loss_sum/(idx+1), time.time() - start))\n",
    "\n",
    "        \"\"\"验证模型\"\"\"\n",
    "        model.eval()\n",
    "        acc = evaluate(model, valid_loader, device)  # 验证模型的性能\n",
    "        ## 保存最优模型\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            torch.save(model.state_dict(), \"best_bert_model.pth\") \n",
    "        \n",
    "        print(\"current acc is {:.4f}, best acc is {:.4f}\".format(acc, best_acc))\n",
    "        print(\"time costed = {}s \\n\".format(round(time.time() - start, 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8ff0198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Running training epoch 1 *****\n",
      "Epoch 0001 | Step 0029/0148 | Loss 0.7052 | Time 25.4793\n",
      "Epoch 0001 | Step 0058/0148 | Loss 0.6583 | Time 50.0558\n",
      "Epoch 0001 | Step 0087/0148 | Loss 0.6445 | Time 74.7011\n",
      "Epoch 0001 | Step 0116/0148 | Loss 0.6144 | Time 99.7510\n",
      "Epoch 0001 | Step 0145/0148 | Loss 0.6077 | Time 124.5971\n",
      "current acc is 0.6351, best acc is 0.6351\n",
      "time costed = 130.31814s \n",
      "\n",
      "***** Running training epoch 2 *****\n",
      "Epoch 0002 | Step 0029/0148 | Loss 0.4953 | Time 24.5054\n",
      "Epoch 0002 | Step 0058/0148 | Loss 0.4706 | Time 49.0399\n",
      "Epoch 0002 | Step 0087/0148 | Loss 0.4543 | Time 73.1791\n",
      "Epoch 0002 | Step 0116/0148 | Loss 0.4804 | Time 96.0788\n",
      "Epoch 0002 | Step 0145/0148 | Loss 0.4780 | Time 118.7552\n",
      "current acc is 0.9189, best acc is 0.9189\n",
      "time costed = 124.17217s \n",
      "\n",
      "***** Running training epoch 3 *****\n",
      "Epoch 0003 | Step 0029/0148 | Loss 0.3980 | Time 23.4422\n",
      "Epoch 0003 | Step 0058/0148 | Loss 0.3732 | Time 48.9625\n",
      "Epoch 0003 | Step 0087/0148 | Loss 0.4004 | Time 73.5836\n",
      "Epoch 0003 | Step 0116/0148 | Loss 0.4119 | Time 98.2131\n",
      "Epoch 0003 | Step 0145/0148 | Loss 0.4326 | Time 122.8648\n",
      "current acc is 0.8649, best acc is 0.9189\n",
      "time costed = 128.15147s \n",
      "\n",
      "***** Running training epoch 4 *****\n",
      "Epoch 0004 | Step 0029/0148 | Loss 0.4424 | Time 24.4487\n",
      "Epoch 0004 | Step 0058/0148 | Loss 0.4011 | Time 48.7206\n",
      "Epoch 0004 | Step 0087/0148 | Loss 0.3839 | Time 73.4749\n",
      "Epoch 0004 | Step 0116/0148 | Loss 0.3619 | Time 98.1080\n",
      "Epoch 0004 | Step 0145/0148 | Loss 0.3588 | Time 122.2778\n",
      "current acc is 0.8514, best acc is 0.9189\n",
      "time costed = 127.93074s \n",
      "\n",
      "***** Running training epoch 5 *****\n",
      "Epoch 0005 | Step 0029/0148 | Loss 0.3557 | Time 24.5972\n",
      "Epoch 0005 | Step 0058/0148 | Loss 0.3171 | Time 48.2247\n",
      "Epoch 0005 | Step 0087/0148 | Loss 0.3577 | Time 71.7677\n",
      "Epoch 0005 | Step 0116/0148 | Loss 0.3478 | Time 95.1323\n",
      "Epoch 0005 | Step 0145/0148 | Loss 0.3351 | Time 118.7383\n",
      "current acc is 0.8378, best acc is 0.9189\n",
      "time costed = 124.16313s \n",
      "\n",
      "***** Running training epoch 6 *****\n",
      "Epoch 0006 | Step 0029/0148 | Loss 0.3385 | Time 23.8689\n",
      "Epoch 0006 | Step 0058/0148 | Loss 0.2969 | Time 47.7130\n",
      "Epoch 0006 | Step 0087/0148 | Loss 0.2712 | Time 71.5543\n",
      "Epoch 0006 | Step 0116/0148 | Loss 0.2910 | Time 95.9228\n",
      "Epoch 0006 | Step 0145/0148 | Loss 0.2711 | Time 120.6101\n",
      "current acc is 0.8243, best acc is 0.9189\n",
      "time costed = 126.12728s \n",
      "\n",
      "***** Running training epoch 7 *****\n",
      "Epoch 0007 | Step 0029/0148 | Loss 0.2082 | Time 24.1766\n",
      "Epoch 0007 | Step 0058/0148 | Loss 0.2320 | Time 48.6762\n",
      "Epoch 0007 | Step 0087/0148 | Loss 0.2134 | Time 73.2673\n",
      "Epoch 0007 | Step 0116/0148 | Loss 0.2197 | Time 95.7599\n",
      "Epoch 0007 | Step 0145/0148 | Loss 0.2160 | Time 118.4373\n",
      "current acc is 0.8378, best acc is 0.9189\n",
      "time costed = 123.56192s \n",
      "\n",
      "***** Running training epoch 8 *****\n",
      "Epoch 0008 | Step 0029/0148 | Loss 0.1834 | Time 22.5731\n",
      "Epoch 0008 | Step 0058/0148 | Loss 0.1777 | Time 45.2287\n",
      "Epoch 0008 | Step 0087/0148 | Loss 0.1584 | Time 67.8703\n",
      "Epoch 0008 | Step 0116/0148 | Loss 0.1628 | Time 90.3733\n",
      "Epoch 0008 | Step 0145/0148 | Loss 0.1724 | Time 112.8998\n",
      "current acc is 0.8243, best acc is 0.9189\n",
      "time costed = 118.04874s \n",
      "\n",
      "***** Running training epoch 9 *****\n",
      "Epoch 0009 | Step 0029/0148 | Loss 0.1111 | Time 23.7340\n",
      "Epoch 0009 | Step 0058/0148 | Loss 0.1122 | Time 47.7679\n",
      "Epoch 0009 | Step 0087/0148 | Loss 0.1044 | Time 71.7243\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtrain_and_eval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 46\u001b[39m, in \u001b[36mtrain_and_eval\u001b[39m\u001b[34m(model, train_loader, valid_loader, optimizer, scheduler, device, epoch)\u001b[39m\n\u001b[32m     44\u001b[39m loss = criterion(y_pred, y)\n\u001b[32m     45\u001b[39m optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     47\u001b[39m optimizer.step()\n\u001b[32m     48\u001b[39m scheduler.step()   \u001b[38;5;66;03m# 学习率变化\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Conda\\envs\\test\\Lib\\site-packages\\torch\\_tensor.py:648\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    640\u001b[39m         Tensor.backward,\n\u001b[32m    641\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    646\u001b[39m         inputs=inputs,\n\u001b[32m    647\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Conda\\envs\\test\\Lib\\site-packages\\torch\\autograd\\__init__.py:353\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    348\u001b[39m     retain_graph = create_graph\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Conda\\envs\\test\\Lib\\site-packages\\torch\\autograd\\graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "train_and_eval(model, train_loader, valid_loader, optimizer, scheduler, DEVICE, EPOCHS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a032b83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Test Accuracy = 0.85 \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9677    0.7317    0.8333        41\n",
      "           1     0.7755    0.9744    0.8636        39\n",
      "\n",
      "    accuracy                         0.8500        80\n",
      "   macro avg     0.8716    0.8530    0.8485        80\n",
      "weighted avg     0.8740    0.8500    0.8481        80\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"best_bert_model.pth\"))\n",
    "pred_test = predict(model, test_loader, DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65cd59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = \"html head meta meta meta meta meta meta link meta link link link link link link title meta meta meta meta meta meta link link meta meta meta meta meta meta meta meta meta meta meta meta meta meta meta meta meta script script link meta meta meta meta meta meta body div section div div div script span a div h1 span img a a nav ul li a ul li a li a li a li a li a li a li a li a li a li a li a li a li a li a ul li a li a li a li a li a li a li a ul li a li a li a li a li a li a li a li a li a li a li a li a ul li a li a li a li a li a li span section div div img img div div div div div div h1 span p input a p a section div div div div div div div div div div h2 p br br p a section div div div div div div div div div div h2 p p p a footer section div div div div ul li a li a li a li a span div ul li a span span a span span a span span section div div div h5 ul li a li a li a li a li a li a li a li a li a li a li a li a div h5 ul li a li a li a li a li a li a div h5 ul li a li a li a li a li a li a li a li a li a li a li a div h5 ul li a li a li a li a li a div a div div br br p b hr div div small span span a a a small a a a section a div div div h6 section div div div div div h3 p a script script script script script script script script noscript img script script script script\"\n",
    "\n",
    "encode_dict = tokenizer.encode_plus(text=tags, max_length=MAX_LEN,\n",
    "                                                padding=\"max_length\", truncation=True)\n",
    "\n",
    "input_ids = encode_dict[\"input_ids\"]\n",
    "input_types = encode_dict[\"token_type_ids\"]\n",
    "input_masks = encode_dict[\"attention_mask\"]\n",
    "result = model(torch.LongTensor([input_ids, ]), torch.LongTensor([input_types, ]), torch.LongTensor([input_masks, ]))\n",
    "torch.argmax(result, dim=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
