{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4b66e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from transformers import BertModel, BertTokenizer, BertConfig, get_cosine_schedule_with_warmup\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
    "from torch.optim import AdamW\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "BERT_PATH = \"bert_model/\"    # 该文件夹下存放三个文件（'vocab.txt', 'pytorch_model.bin', 'config.json'）\n",
    "# DATA_PATH = \"data/tags_data.txt\"\n",
    "DATA_PATH = \"data/test_data.txt\"\n",
    "MAX_LEN = 64\n",
    "BATCH_SIZE = 4\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33e36253",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bert_Model(nn.Module):\n",
    "    def __init__(self, bert_path, classes=2):\n",
    "        super(Bert_Model, self).__init__()\n",
    "        self.config = BertConfig.from_pretrained(bert_path)  # 导入模型超参数\n",
    "        self.bert = BertModel.from_pretrained(bert_path)     # 加载预训练模型权重\n",
    "        self.fc = nn.Linear(self.config.hidden_size, classes)  # 直接分类\n",
    "        \n",
    "        \n",
    "    def forward(self, input_ids, attention_mask=None, token_type_ids=None):\n",
    "        outputs = self.bert(input_ids, attention_mask, token_type_ids)\n",
    "        out_pool = outputs[1]   # 池化\n",
    "        logit = self.fc(out_pool) # 线性模型二分类\n",
    "        return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35e41f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(BERT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe7dc1c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "746it [00:01, 610.47it/s]\n"
     ]
    }
   ],
   "source": [
    "def load_data():\n",
    "    input_ids, input_masks, input_types, tag_labels = [], [], [], []\n",
    "\n",
    "    with open(DATA_PATH, encoding=\"utf-8\") as f:\n",
    "        for line in tqdm(f):\n",
    "            tags, labels = line.strip().split(\"\\t\")\n",
    "            encode_dict = tokenizer.encode_plus(text=tags, max_length=MAX_LEN,\n",
    "                                                    padding=\"max_length\", truncation=True)\n",
    "\n",
    "            input_ids.append(encode_dict[\"input_ids\"])\n",
    "            input_types.append(encode_dict[\"token_type_ids\"])\n",
    "            input_masks.append(encode_dict[\"attention_mask\"])\n",
    "            tag_labels.append(int(labels))\n",
    "\n",
    "    all_data = (input_ids, input_masks, input_types, tag_labels)\n",
    "    unit = len(tag_labels) // 10\n",
    "    train_data = list(map(lambda x: x[:unit*8], all_data))\n",
    "    valid_data = list(map(lambda x: x[unit*8:unit*9], all_data))\n",
    "    test_data = list(map(lambda x: x[unit*9:], all_data))\n",
    "\n",
    "    return train_data, valid_data, test_data\n",
    "train_data, valid_data, test_data = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20512bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(*tuple(map(torch.LongTensor, train_data)))\n",
    "train_sampler = RandomSampler(train_dataset)  \n",
    "train_loader = DataLoader(train_dataset, sampler=train_sampler, batch_size=BATCH_SIZE)\n",
    "\n",
    "valid_dataset = TensorDataset(*tuple(map(torch.LongTensor, valid_data)))\n",
    "valid_sampler = RandomSampler(valid_dataset)  \n",
    "valid_loader = DataLoader(valid_dataset, sampler=valid_sampler, batch_size=BATCH_SIZE)\n",
    "\n",
    "test_dataset = TensorDataset(*tuple(map(torch.LongTensor, test_data)))\n",
    "test_sampler = RandomSampler(test_dataset)  \n",
    "test_loader = DataLoader(test_dataset, sampler=test_sampler, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "437a681e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert_model/ were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 102269186, Trainable parameters: 102269186\n"
     ]
    }
   ],
   "source": [
    "def get_parameter_number(model):\n",
    "    #  打印模型参数量\n",
    "    total_num = sum(p.numel() for p in model.parameters())\n",
    "    trainable_num = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return 'Total parameters: {}, Trainable parameters: {}'.format(total_num, trainable_num)\n",
    "\n",
    "# DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "DEVICE = torch.device(\"cpu\")\n",
    "model = Bert_Model(BERT_PATH).to(DEVICE)\n",
    "print(get_parameter_number(model))\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=1e-4) #AdamW优化器\n",
    "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=len(train_loader),\n",
    "                                            num_training_steps=EPOCHS*len(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98dfcb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 评估模型性能，在验证集上\n",
    "def evaluate(model, data_loader, device):\n",
    "    model.eval()\n",
    "    val_true, val_pred = [], []\n",
    "    with torch.no_grad():\n",
    "        for idx, (ids, att, tpe, y) in (enumerate(data_loader)):\n",
    "            y_pred = model(ids.to(device), att.to(device), tpe.to(device))\n",
    "            y_pred = torch.argmax(y_pred, dim=1).detach().cpu().numpy().tolist()\n",
    "            val_pred.extend(y_pred)\n",
    "            val_true.extend(y.squeeze().cpu().numpy().tolist())\n",
    "    \n",
    "    return accuracy_score(val_true, val_pred)  #返回accuracy\n",
    "\n",
    "\n",
    "# 测试集没有标签，需要预测提交\n",
    "def predict(model, data_loader, device):\n",
    "    model.eval()\n",
    "    val_true, val_pred = [], []\n",
    "    with torch.no_grad():\n",
    "        for idx, (ids, att, tpe, y) in tqdm(enumerate(data_loader)):\n",
    "            y_pred = model(ids.to(device), att.to(device), tpe.to(device))\n",
    "            y_pred = torch.argmax(y_pred, dim=1).detach().cpu().numpy().tolist()\n",
    "            val_pred.extend(y_pred)\n",
    "            val_true.extend(y.squeeze().cpu().numpy().tolist())\n",
    "\n",
    "    print(\"\\n Test Accuracy = {} \\n\".format(accuracy_score(val_true, val_pred)))\n",
    "    print(classification_report(val_true, val_pred, digits=4))\n",
    "\n",
    "\n",
    "def train_and_eval(model, train_loader, valid_loader, \n",
    "                   optimizer, scheduler, device, epoch):\n",
    "    best_acc = 0.0\n",
    "    patience = 0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    for i in range(epoch):\n",
    "        \"\"\"训练模型\"\"\"\n",
    "        start = time.time()\n",
    "        model.train()\n",
    "        print(\"***** Running training epoch {} *****\".format(i+1))\n",
    "        train_loss_sum = 0.0\n",
    "        for idx, (ids, att, tpe, y) in enumerate(train_loader):\n",
    "            ids, att, tpe, y = ids.to(device), att.to(device), tpe.to(device), y.to(device)  \n",
    "            y_pred = model(ids, att, tpe)\n",
    "            loss = criterion(y_pred, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()   # 学习率变化\n",
    "            \n",
    "            train_loss_sum += loss.item()\n",
    "            if (idx + 1) % (len(train_loader)//5) == 0:    # 只打印五次结果\n",
    "                print(\"Epoch {:04d} | Step {:04d}/{:04d} | Loss {:.4f} | Time {:.4f}\".format(\n",
    "                          i+1, idx+1, len(train_loader), train_loss_sum/(idx+1), time.time() - start))\n",
    "\n",
    "        \"\"\"验证模型\"\"\"\n",
    "        model.eval()\n",
    "        acc = evaluate(model, valid_loader, device)  # 验证模型的性能\n",
    "        ## 保存最优模型\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            torch.save(model.state_dict(), \"best_bert_model.pth\") \n",
    "        \n",
    "        print(\"current acc is {:.4f}, best acc is {:.4f}\".format(acc, best_acc))\n",
    "        print(\"time costed = {}s \\n\".format(round(time.time() - start, 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ff0198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Running training epoch 1 *****\n",
      "Epoch 0001 | Step 0029/0148 | Loss 0.6828 | Time 23.4279\n",
      "Epoch 0001 | Step 0058/0148 | Loss 0.6533 | Time 47.0413\n",
      "Epoch 0001 | Step 0087/0148 | Loss 0.6363 | Time 71.2311\n",
      "Epoch 0001 | Step 0116/0148 | Loss 0.6157 | Time 94.7125\n",
      "Epoch 0001 | Step 0145/0148 | Loss 0.6045 | Time 118.2316\n",
      "current acc is 0.8108, best acc is 0.8108\n",
      "time costed = 123.97417s \n",
      "\n",
      "***** Running training epoch 2 *****\n",
      "Epoch 0002 | Step 0029/0148 | Loss 0.6089 | Time 24.1157\n",
      "Epoch 0002 | Step 0058/0148 | Loss 0.5195 | Time 48.0619\n",
      "Epoch 0002 | Step 0087/0148 | Loss 0.4965 | Time 71.6794\n",
      "Epoch 0002 | Step 0116/0148 | Loss 0.5054 | Time 95.7201\n",
      "Epoch 0002 | Step 0145/0148 | Loss 0.5024 | Time 119.4182\n",
      "current acc is 0.7838, best acc is 0.8108\n",
      "time costed = 124.88924s \n",
      "\n",
      "***** Running training epoch 3 *****\n",
      "Epoch 0003 | Step 0029/0148 | Loss 0.4326 | Time 25.6443\n",
      "Epoch 0003 | Step 0058/0148 | Loss 0.4461 | Time 49.3072\n",
      "Epoch 0003 | Step 0087/0148 | Loss 0.4813 | Time 73.9449\n",
      "Epoch 0003 | Step 0116/0148 | Loss 0.4728 | Time 98.2020\n",
      "Epoch 0003 | Step 0145/0148 | Loss 0.4746 | Time 121.3465\n",
      "current acc is 0.7027, best acc is 0.8108\n",
      "time costed = 126.6404s \n",
      "\n",
      "***** Running training epoch 4 *****\n",
      "Epoch 0004 | Step 0029/0148 | Loss 0.3713 | Time 23.7210\n",
      "Epoch 0004 | Step 0058/0148 | Loss 0.4159 | Time 47.3429\n",
      "Epoch 0004 | Step 0087/0148 | Loss 0.4216 | Time 71.2832\n",
      "Epoch 0004 | Step 0116/0148 | Loss 0.4438 | Time 94.4525\n",
      "Epoch 0004 | Step 0145/0148 | Loss 0.4353 | Time 117.4241\n",
      "current acc is 0.8919, best acc is 0.8919\n",
      "time costed = 123.15816s \n",
      "\n",
      "***** Running training epoch 5 *****\n",
      "Epoch 0005 | Step 0029/0148 | Loss 0.2985 | Time 23.0635\n",
      "Epoch 0005 | Step 0058/0148 | Loss 0.3415 | Time 45.8940\n",
      "Epoch 0005 | Step 0087/0148 | Loss 0.3714 | Time 68.8409\n",
      "Epoch 0005 | Step 0116/0148 | Loss 0.3690 | Time 92.0000\n",
      "Epoch 0005 | Step 0145/0148 | Loss 0.3718 | Time 115.3295\n",
      "current acc is 0.7973, best acc is 0.8919\n",
      "time costed = 120.95261s \n",
      "\n",
      "***** Running training epoch 6 *****\n",
      "Epoch 0006 | Step 0029/0148 | Loss 0.2985 | Time 24.0271\n",
      "Epoch 0006 | Step 0058/0148 | Loss 0.3765 | Time 47.5535\n",
      "Epoch 0006 | Step 0087/0148 | Loss 0.3442 | Time 71.0506\n",
      "Epoch 0006 | Step 0116/0148 | Loss 0.2993 | Time 94.5579\n",
      "Epoch 0006 | Step 0145/0148 | Loss 0.2909 | Time 118.2038\n",
      "current acc is 0.7973, best acc is 0.8919\n",
      "time costed = 123.59483s \n",
      "\n",
      "***** Running training epoch 7 *****\n",
      "Epoch 0007 | Step 0029/0148 | Loss 0.3163 | Time 23.8522\n",
      "Epoch 0007 | Step 0058/0148 | Loss 0.2635 | Time 48.2420\n",
      "Epoch 0007 | Step 0087/0148 | Loss 0.2604 | Time 71.7481\n"
     ]
    }
   ],
   "source": [
    "train_and_eval(model, train_loader, valid_loader, optimizer, scheduler, DEVICE, EPOCHS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a032b83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Test Accuracy = 0.85 \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9677    0.7317    0.8333        41\n",
      "           1     0.7755    0.9744    0.8636        39\n",
      "\n",
      "    accuracy                         0.8500        80\n",
      "   macro avg     0.8716    0.8530    0.8485        80\n",
      "weighted avg     0.8740    0.8500    0.8481        80\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"best_bert_model.pth\"))\n",
    "pred_test = predict(model, test_loader, DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65cd59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = \"html head meta meta meta meta meta meta link meta link link link link link link title meta meta meta meta meta meta link link meta meta meta meta meta meta meta meta meta meta meta meta meta meta meta meta meta script script link meta meta meta meta meta meta body div section div div div script span a div h1 span img a a nav ul li a ul li a li a li a li a li a li a li a li a li a li a li a li a li a li a ul li a li a li a li a li a li a li a ul li a li a li a li a li a li a li a li a li a li a li a li a ul li a li a li a li a li a li span section div div img img div div div div div div h1 span p input a p a section div div div div div div div div div div h2 p br br p a section div div div div div div div div div div h2 p p p a footer section div div div div ul li a li a li a li a span div ul li a span span a span span a span span section div div div h5 ul li a li a li a li a li a li a li a li a li a li a li a li a div h5 ul li a li a li a li a li a li a div h5 ul li a li a li a li a li a li a li a li a li a li a li a div h5 ul li a li a li a li a li a div a div div br br p b hr div div small span span a a a small a a a section a div div div h6 section div div div div div h3 p a script script script script script script script script noscript img script script script script\"\n",
    "\n",
    "encode_dict = tokenizer.encode_plus(text=tags, max_length=MAX_LEN,\n",
    "                                                padding=\"max_length\", truncation=True)\n",
    "\n",
    "input_ids = encode_dict[\"input_ids\"]\n",
    "input_types = encode_dict[\"token_type_ids\"]\n",
    "input_masks = encode_dict[\"attention_mask\"]\n",
    "result = model(torch.LongTensor([input_ids, ]), torch.LongTensor([input_types, ]), torch.LongTensor([input_masks, ]))\n",
    "torch.argmax(result, dim=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
